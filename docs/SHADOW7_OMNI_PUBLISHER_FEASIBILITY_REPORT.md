# Shadow-7 Omni-Publisher â€” Feasibility Report & Architecture

**Authority:** Senior AI System Architect & Sovereign Integrator  
**Project:** Shadow-7 Omni-Publisher (The Ultimate Ingestion & Publishing Pipeline)  
**Date:** February 27, 2026  
**Classification:** Feasibility Report + Microservices Map for Stages 1â€“2

---

## Executive Summary

| Criterion | Verdict | Notes |
|-----------|---------|-------|
| **Overall Feasibility** | âœ… **FEASIBLE** | All 7 pillars are technically achievable with existing or well-documented tooling |
| **RTL/Arabic Integrity** | âœ… **ACHIEVABLE** | ReportLab 4.4+, python-docx with bidi, LaTeX/XeLaTeX â€” all support Arabic RTL |
| **Long Context (200k words)** | âœ… **VIABLE** | Gemini 1.5 Pro (2M tokens) can ingest ~200k words; hybrid with local RAG recommended |
| **NEXUS PRIME Integration** | âœ… **ALIGNED** | Shadow-7 backend (8002), nexus_oracle, sovereign_dify_bridge, manuscripts DB already exist |
| **Risk Level** | ğŸŸ¡ **MEDIUM** | Main risks: Arabic PDF edge cases, API cost at scale, thematic consistency for 16 images |

---

## 1. Input Specifications â€” Validation

| Input | Requirement | Status | Implementation Notes |
|-------|--------------|--------|----------------------|
| **Sources** | 1â€“7 TXT files | âœ… | Already supported: `.txt` + `.docx` in `main.py` upload; extend to multi-file merge |
| **Volume** | Up to 200,000 words | âœ… | `MAX_MANUSCRIPT_UPLOAD=100MB` exists; 200k words â‰ˆ 1.2 MB â€” within limits |
| **RTL/RTS Integrity** | 100% throughout | âœ… | UTF-8 + `scrub_text` + `arabicTokenizer.js`; must enforce at every stage |

**Current Gaps:**  
- `MAX_WORDS=3000` in config is too low for 200k target â€” update to `MAX_WORDS=200000` or `MAX_WORDS=250000` for safety.  
- Multi-file (1â€“7 TXT) merge is not yet implemented â€” add in Stage 1.

---

## 2. The 7 Pillars â€” Feasibility Assessment

### Pillar 1: Intake & Validation

| Task | Feasibility | Notes |
|------|-------------|-------|
| Pre-processing TXT | âœ… | `scrub_text` + `arabicTokenizer.js`; add `encoding` detection (UTF-8, CP1256, ISO-8859-6) |
| Word count | âœ… | `count_words` in main.py; `wordCount` in arabicTokenizer.js |
| Language encoding | âœ… | `chardet` or `ftfy` for Arabic; validate Unicode normalization (NFC) |

**Recommendation:** Extend existing `/api/shadow7/upload` and `/api/shadow7/manuscripts/upload` to accept 1â€“7 files, merge content, validate total word count.

---

### Pillar 2: Semantic Purge

| Task | Feasibility | Notes |
|------|-------------|-------|
| Duplicate detection | âœ… | Semantic similarity (embeddings) or n-gram; use `sentence-transformers` or `faiss` |
| Thematic outliers | âœ… | Cluster analysis on embeddings; flag outliers beyond threshold |
| Content anomalies | âœ… | LLM-based anomaly detection (e.g., off-topic, mixed languages, spam) |

**Recommendation:** Use Gemini 1.5 Pro for long-context analysis (see Section 3 below) or local embeddings + clustering. Output: purge report + cleaned text.

---

### Pillar 3: Nuwa (Skeletal) Generation

| Task | Feasibility | Notes |
|------|-------------|-------|
| Cover placeholder | âœ… | Template with title, author, metadata |
| Table of Contents | âœ… | Extract headings from body; LLM or rule-based |
| Introduction | âœ… | LLM-generated from summary |
| Body structure | âœ… | Chapter/section extraction from purge output |
| Index | âœ… | Keyword extraction (arabicTokenizer + TF-IDF) |
| References | âœ… | Pattern matching for citations; BibTeX-style if needed |

**Recommendation:** Define JSON schema for skeleton; generate via LLM or hybrid rules. Output: structured JSON (Nuwa schema).

---

### Pillar 4: Sovereign Expansion

| Task | Feasibility | Notes |
|------|-------------|-------|
| 100kâ€“200k target | âœ… | Chunked expansion with LLM; track word count per chunk |
| Gemini 1.5 Pro (analysis) | âœ… | 2M context window; ~200k words â‰ˆ 250k tokens â€” fits |
| Local LLMs (generation) | âœ… | Ollama/LiteLLM already in NEXUS; llama3.2, mistral, etc. |

**Recommendation:**  
- **Analysis:** Gemini 1.5 Pro for full-document semantic analysis, outline extraction, theme mapping.  
- **Generation:** Local Ollama/LiteLLM for chunk-by-chunk expansion (cost control, privacy).  
- **RAG:** Optional â€” use for retrieval of relevant chunks during expansion; not required for analysis if Gemini can ingest full text.

---

### Pillar 5: Compliance & Market Scan

| Task | Feasibility | Notes |
|------|-------------|-------|
| Regional regulations | âœ… | LLM + curated knowledge base (e.g., Saudi, UAE, Egypt publishing rules) |
| User-country specific | âœ… | User profile or request param: `country_code` |
| Market entry strategy | âœ… | LLM-generated report from regulations + genre + audience |

**Recommendation:** Integrate with NEXUS PRIME Compliance Shield (see `IDENTITY_COMPLIANCE_PROTOCOL_KIER.md`). Add country-specific compliance rules to Dify or nexus_oracle.

---

### Pillar 6: Creative Suite

| Task | Feasibility | Notes |
|------|-------------|-------|
| 4 titles | âœ… | LLM generation |
| 3 professional covers | âœ… | DALL-E / Midjourney / Stable Diffusion API; RTL text overlay via image lib |
| 16 social assets (4Ã—4) | âœ… | Same style via seed + style reference (see Section 3) |
| 2 promotional videos | âœ… | Text-to-video (Runway, Pika) or slideshow + audio |

**Thematic Consistency:**  
- Use **style reference** (Midjourney `--style reference` or `--cref`).  
- Generate **thematic brief** from manuscript (LLM): 5â€“10 core themes, visual style, mood.  
- Lock **seed** and **style parameters** across all 16 images.  
- Use **modular prompt template:** `[Scene]; [Location]; [Lighting]; [Art style]; [Parameters]` â€” vary only scene/location.

---

### Pillar 7: Encapsulation

| Task | Feasibility | Notes |
|------|-------------|-------|
| 25 distinct files | âœ… | ZIP via Python `zipfile` or `JSZip` (already in assets) |
| Word | âœ… | python-docx with RTL (see Section 3) |
| Print-ready PDF | âœ… | ReportLab 4.4+ or LaTeX/XeLaTeX |
| High-res images | âœ… | PNG/JPG from Pillar 6 |
| MP4 videos | âœ… | From Pillar 6 |

---

## 3. Technical Constraints â€” Answers

### 3.1 Long Context: Gemini 1.5 Pro vs Local RAG

| Approach | Pros | Cons |
|----------|------|------|
| **Gemini 1.5 Pro (2M context)** | Single-pass full-document analysis; >99% retrieval recall; no chunking loss | API cost; latency for 200k words â‰ˆ 1â€“2 min |
| **Local RAG** | Cost-effective; privacy; incremental | Chunking can lose cross-chapter coherence; retrieval may miss distant context |

**Recommendation:**  
- **Analysis:** Gemini 1.5 Pro for semantic purge, outline extraction, theme mapping. One API call per document.  
- **Generation:** Local RAG + Ollama for chunk-by-chunk expansion. RAG retrieves relevant prior sections + outline for context.

---

### 3.2 Arabic PDF/Word Generation â€” Zero RTL Collapse

| Library | RTL Support | Recommendation |
|---------|-------------|----------------|
| **ReportLab 4.4+** | âœ… Experimental Arabic; HarfBuzz; `rlbidi`; `wordWrap="RTL"` | Use for programmatic PDF; validate with real Arabic text |
| **LaTeX/XeLaTeX** | âœ… `babel` + `polyglossia` + `arabtex`; industry standard for Arabic typography | Best for print-ready; requires `xelatex` + Arabic fonts |
| **python-docx** | âœ… `w:bidi` + `w:rtl` on run/paragraph; custom style | Use for Word; set `font.complex_script = True` |
| **Node.js (puppeteer + HTML)** | âœ… HTML `dir="rtl"` + CSS; render to PDF | Fallback; good for web-like layouts |

**Recommendation:**  
- **Word:** python-docx with bidi/rtl; Amiri or Noto Naskh Arabic font.  
- **PDF:** LaTeX/XeLaTeX for highest quality; ReportLab 4.4+ as fallback if LaTeX not desired.  
- **Fonts:** Amiri, Noto Naskh Arabic, or Tahoma â€” ensure embedded in PDF/Word.

---

### 3.3 NEXUS PRIME & Sovereign Master Context Integration

| Component | Role | Integration |
|-----------|------|-------------|
| **nexus_oracle** (8100) | RAG for Sovereign Encyclopedia | Query for compliance rules, governance axioms |
| **sovereign_dify_bridge** (8888) | Dify orchestration | Route expansion, compliance, creative prompts |
| **nexus_db** | PostgreSQL | manuscripts, requests, logs â€” already used by Shadow-7 |
| **shadow7_api** (8002) | FastAPI backend | Extend with new stages; keep `/api/shadow7/` prefix |
| **Sovereign Master Context** | ENTERPRISE_CODEX, SOVEREIGN_ENCYCLOPEDIA | Inject into Dify prompts for governance-aware decisions |

**Sync Points:**  
- Manuscript metadata â†’ nexus_db.manuscripts  
- Compliance check â†’ sovereign_dify_bridge + nexus_oracle  
- Status/logs â†’ existing `db.log` and `request_status` flow

---

### 3.4 Thematic Consistency: 16 Images â†” 200k Words

**Strategy:**

1. **Thematic Brief (LLM):** Extract from manuscript: 5 themes, 3 visual style keywords, mood, color palette.  
2. **Style Prompt Template:** Lock `[Art style]; [Lighting]; [Parameters]`; vary only `[Scene]` per platform.  
3. **Style Reference:** Generate 1 "master" image; use as `--cref` for remaining 15.  
4. **Seed Lock:** Same seed across batch when API supports it (Stable Diffusion, Midjourney).  

**Output:** 4 platforms Ã— 4 images = 16 assets; each platform gets 4 variations of same scene/style.

---

## 4. Microservices Map â€” Stages 1 & 2

### 4.1 Stage 1: Intake & Validation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STAGE 1: INTAKE & VALIDATION                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  [Client]  â”€â”€â–º  POST /api/shadow7/omni/upload
                      â”‚
                      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  shadow7_intake (NEW)                                                    â”‚
  â”‚  â€¢ Accept 1â€“7 TXT/DOCX files (multipart/form-data)                       â”‚
  â”‚  â€¢ Merge content; preserve order (file_1, file_2, ...)                     â”‚
  â”‚  â€¢ Detect encoding: UTF-8, CP1256, ISO-8859-6                            â”‚
  â”‚  â€¢ Normalize: NFC, scrub_text, arabicTokenizer.normalizeArabic            â”‚
  â”‚  â€¢ Validate: 500 â‰¤ total_words â‰¤ 200,000                                 â”‚
  â”‚  â€¢ Store: manuscripts table + file_path(s)                               â”‚
  â”‚  â€¢ Return: { tracking_id, word_count, file_count, encoding }               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
  [nexus_db]  manuscripts, requests
```

**Microservices:**

| Service | Responsibility | Port |
|---------|----------------|------|
| **shadow7_api** (existing) | Add `/api/shadow7/omni/upload`; orchestrate intake | 8002 |
| **nexus_db** | manuscripts, requests, logs | 5432 |

**New Files to Create:**

- `backend/services/intake_service.py` â€” `IntakeService.merge_and_validate(files)`
- `backend/routes/omni_routes.py` â€” `/omni/upload` handler
- Update `config.py`: `MAX_WORDS=200000`, `MAX_OMNI_FILES=7`

---

### 4.2 Stage 2: Semantic Purge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STAGE 2: SEMANTIC PURGE                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  [Intake Output]  â”€â”€â–º  POST /api/shadow7/omni/purge
                              â”‚
                              â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  shadow7_purge (NEW)                                                     â”‚
  â”‚  â€¢ Input: tracking_id, manuscript_id                                     â”‚
  â”‚  â€¢ Load: full text from manuscripts                                      â”‚
  â”‚  â€¢ Option A: Gemini 1.5 Pro API â€” full-doc analysis                      â”‚
  â”‚    - Prompt: "Identify duplicates, thematic outliers, anomalies"         â”‚
  â”‚    - Output: structured JSON (duplicates, outliers, purge_actions)       â”‚
  â”‚  â€¢ Option B: Local embeddings (sentence-transformers) + clustering       â”‚
  â”‚    - Embed paragraphs; cluster; flag outliers                            â”‚
  â”‚  â€¢ Apply purge: remove/merge per rules                                   â”‚
  â”‚  â€¢ Store: purge_report, purged_text in manuscripts or new table          â”‚
  â”‚  â€¢ Return: { purge_report, word_count_after, anomalies_fixed }            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
  [nexus_db]  manuscripts.purge_report, manuscripts.purged_content
  [Gemini API]  (if Option A)
  [Ollama]  (if Option B â€” local embeddings)
```

**Microservices:**

| Service | Responsibility | Port |
|---------|----------------|------|
| **shadow7_api** | Add `/api/shadow7/omni/purge`; orchestrate purge | 8002 |
| **nexus_db** | purge_report, purged_content | 5432 |
| **Gemini 1.5 Pro** (external) | Full-doc analysis (Option A) | API |
| **nexus_ollama** | Local embeddings (Option B) | 11434 |

**New Files to Create:**

- `backend/services/purge_service.py` â€” `PurgeService.analyze(text)` and `PurgeService.apply(text, report)`
- `backend/services/gemini_client.py` â€” Gemini 1.5 Pro API client (if Option A)
- `backend/routes/omni_routes.py` â€” `/omni/purge` handler
- DB migration: `manuscripts.purge_report` (JSONB), `manuscripts.purged_content` (TEXT)

---

### 4.3 End-to-End Flow (Stages 1â€“2)

```
  [User]  â”€â”€â–º  Upload 1â€“7 TXT  â”€â”€â–º  Intake  â”€â”€â–º  Purge  â”€â”€â–º  [Purged Manuscript]
                â”‚                    â”‚              â”‚
                â”‚                    â”‚              â””â”€â”€â–º  Gemini 1.5 Pro
                â”‚                    â”‚                    or Local Embeddings
                â”‚                    â”‚
                â”‚                    â””â”€â”€â–º  nexus_db
                â”‚
                â””â”€â”€â–º  manuscripts table
```

---

## 5. Implementation Roadmap

### Phase 1: Stage 1 (Intake) â€” 2â€“3 days

1. Add `IntakeService` with multi-file merge and encoding detection.  
2. Add `/api/shadow7/omni/upload` route.  
3. Update `config.py` limits.  
4. Add tests for 1â€“7 files, encoding edge cases.

### Phase 2: Stage 2 (Purge) â€” 3â€“5 days

1. Add `PurgeService` with Gemini 1.5 Pro client (Option A).  
2. Add `/api/shadow7/omni/purge` route.  
3. DB migration for purge_report, purged_content.  
4. Optional: Local embeddings path (Option B) for cost/privacy.  
5. Add tests for duplicate/outlier detection.

### Phase 3: NEXUS PRIME Wiring

1. Register omni routes in Shadow-7 main app.  
2. Add n8n workflow for omni pipeline (optional).  
3. Document sovereign_dify_bridge integration for future stages.

---

## 6. Appendix: Key File References

| File | Purpose |
|------|---------|
| `backend/main.py` | Gatekeeper, upload, manuscripts; scrub_text, count_words |
| `backend/config.py` | MIN_WORDS, MAX_WORDS, MAX_MANUSCRIPT_UPLOAD |
| `utils/nlp/arabicTokenizer.js` | normalizeArabic, tokenize, wordCount |
| `NEXUS_PRIME_UNIFIED/docs/IDENTITY_COMPLIANCE_PROTOCOL_KIER.md` | Compliance Shield |
| `NEXUS_PRIME_UNIFIED/docs/SOVEREIGN_ENCYCLOPEDIA.md` | RAG source for governance |
| `NEXUS_PRIME_UNIFIED/docker-compose.yml` | nexus_oracle, shadow7_api, nexus_db |

---

**End of Report**
