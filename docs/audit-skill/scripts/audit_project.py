#!/usr/bin/env python3\n\"\"\"\nProject Audit & Gap Analysis Script\nComprehensive analysis of project structure, code quality, and missing components\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef analyze_project_structure(project_path):\n    \"\"\"\n    Analyze the complete project structure\n    Returns: dict with file statistics and organization\n    \"\"\"\n    stats = {\n        'total_files': 0,\n        'by_extension': defaultdict(int),\n        'by_directory': defaultdict(int),\n        'total_lines': 0,\n        'files_list': []\n    }\n    \n    for root, dirs, files in os.walk(project_path):\n        # Skip node_modules, .git, dist, etc\n        dirs[:] = [d for d in dirs if d not in ['node_modules', '.git', 'dist', '.next', 'build']]\n        \n        for file in files:\n            file_path = Path(root) / file\n            stats['total_files'] += 1\n            \n            # Count by extension\n            ext = file_path.suffix or 'no_extension'\n            stats['by_extension'][ext] += 1\n            \n            # Count by directory\n            rel_dir = Path(root).relative_to(project_path).parts[0] if Path(root) != Path(project_path) else 'root'\n            stats['by_directory'][rel_dir] += 1\n            \n            # Count lines for code files\n            if ext in ['.js', '.jsx', '.ts', '.tsx', '.py', '.java', '.go', '.rb', '.php']:\n                try:\n                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                        stats['total_lines'] += len(f.readlines())\n                except:\n                    pass\n            \n            stats['files_list'].append(str(file_path.relative_to(project_path)))\n    \n    return stats\n\ndef check_critical_files(project_path):\n    \"\"\"\n    Check for critical project files\n    Returns: dict with presence of key files\n    \"\"\"\n    critical_files = {\n        'package.json': 'Node.js project',\n        'requirements.txt': 'Python project',\n        'Dockerfile': 'Docker support',\n        'docker-compose.yml': 'Docker Compose',\n        'README.md': 'Documentation',\n        '.env.example': 'Environment config',\n        'tsconfig.json': 'TypeScript',\n        'jest.config.js': 'Jest testing',\n        'vitest.config.js': 'Vitest testing',\n        '.eslintrc': 'ESLint config',\n        'vite.config.js': 'Vite config',\n        'webpack.config.js': 'Webpack config'\n    }\n    \n    found = {}\n    for file, description in critical_files.items():\n        path = Path(project_path) / file\n        found[file] = {\n            'exists': path.exists(),\n            'description': description,\n            'path': str(path) if path.exists() else None\n        }\n    \n    return found\n\ndef analyze_dependencies(project_path):\n    \"\"\"\n    Analyze project dependencies\n    Returns: dict with dependency information\n    \"\"\"\n    deps = {\n        'npm': None,\n        'python': None,\n        'go': None,\n        'total_packages': 0\n    }\n    \n    # Check npm\n    package_json = Path(project_path) / 'package.json'\n    if package_json.exists():\n        try:\n            with open(package_json) as f:\n                data = json.load(f)\n                deps['npm'] = {\n                    'dependencies': len(data.get('dependencies', {})),\n                    'devDependencies': len(data.get('devDependencies', {})),\n                    'total': len(data.get('dependencies', {})) + len(data.get('devDependencies', {}))\n                }\n                deps['total_packages'] += deps['npm']['total']\n        except:\n            pass\n    \n    # Check Python\n    req_file = Path(project_path) / 'requirements.txt'\n    if req_file.exists():\n        try:\n            with open(req_file) as f:\n                deps['python'] = len([l for l in f if l.strip() and not l.startswith('#')])\n                deps['total_packages'] += deps['python']\n        except:\n            pass\n    \n    return deps\n\ndef check_test_coverage(project_path):\n    \"\"\"\n    Check for test files and coverage\n    Returns: dict with test statistics\n    \"\"\"\n    test_stats = {\n        'test_files': 0,\n        'test_directories': [],\n        'test_types': defaultdict(int),\n        'total_test_lines': 0\n    }\n    \n    test_patterns = ['test', 'spec', '__tests__', 'tests']\n    \n    for root, dirs, files in os.walk(project_path):\n        # Check if in test directory\n        if any(pattern in root for pattern in test_patterns):\n            test_stats['test_directories'].append(root)\n            \n            for file in files:\n                if any(pattern in file for pattern in ['test', 'spec']):\n                    test_stats['test_files'] += 1\n                    \n                    # Determine test type\n                    if 'jest' in file or file.endswith('.jest.js'):\n                        test_stats['test_types']['jest'] += 1\n                    elif 'vitest' in file or file.endswith('.vitest.js'):\n                        test_stats['test_types']['vitest'] += 1\n                    elif file.endswith('.py'):\n                        test_stats['test_types']['pytest'] += 1\n                    else:\n                        test_stats['test_types']['other'] += 1\n    \n    return test_stats\n\ndef analyze_code_quality(project_path):\n    \"\"\"\n    Analyze code quality indicators\n    Returns: dict with quality metrics\n    \"\"\"\n    quality = {\n        'has_linting': False,\n        'has_formatting': False,\n        'has_type_checking': False,\n        'has_documentation': False,\n        'linters': [],\n        'formatters': []\n    }\n    \n    # Check for linting\n    linting_files = ['.eslintrc', '.eslintrc.js', '.eslintrc.json', '.pylintrc']\n    for file in linting_files:\n        if (Path(project_path) / file).exists():\n            quality['has_linting'] = True\n            quality['linters'].append(file)\n    \n    # Check for formatting\n    formatting_files = ['.prettierrc', 'prettier.config.js', '.editorconfig']\n    for file in formatting_files:\n        if (Path(project_path) / file).exists():\n            quality['has_formatting'] = True\n            quality['formatters'].append(file)\n    \n    # Check for type checking\n    if (Path(project_path) / 'tsconfig.json').exists():\n        quality['has_type_checking'] = True\n    \n    # Check for documentation\n    doc_files = ['README.md', 'CONTRIBUTING.md', 'docs/', 'documentation/']\n    for file in doc_files:\n        if (Path(project_path) / file).exists():\n            quality['has_documentation'] = True\n            break\n    \n    return quality\n\ndef generate_audit_report(project_path):\n    \"\"\"\n    Generate comprehensive audit report\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PROJECT AUDIT & GAP ANALYSIS REPORT\")\n    print(\"=\"*60 + \"\\n\")\n    \n    # Project structure\n    print(\"ğŸ“Š PROJECT STRUCTURE\")\n    structure = analyze_project_structure(project_path)\n    print(f\"  Total Files: {structure['total_files']}\")\n    print(f\"  Total Lines of Code: {structure['total_lines']:,}\")\n    print(f\"  File Types: {len(structure['by_extension'])}\")\n    print(f\"  Directories: {len(structure['by_directory'])}\")\n    \n    # Critical files\n    print(\"\\nğŸ“ CRITICAL FILES\")\n    critical = check_critical_files(project_path)\n    found_count = sum(1 for f in critical.values() if f['exists'])\n    print(f\"  Found: {found_count}/{len(critical)}\")\n    for file, info in critical.items():\n        status = \"âœ…\" if info['exists'] else \"âŒ\"\n        print(f\"    {status} {file} - {info['description']}\")\n    \n    # Dependencies\n    print(\"\\nğŸ“¦ DEPENDENCIES\")\n    deps = analyze_dependencies(project_path)\n    print(f\"  Total Packages: {deps['total_packages']}\")\n    if deps['npm']:\n        print(f\"  NPM: {deps['npm']['total']} packages\")\n    if deps['python']:\n        print(f\"  Python: {deps['python']} packages\")\n    \n    # Tests\n    print(\"\\nğŸ§ª TESTS\")\n    tests = check_test_coverage(project_path)\n    print(f\"  Test Files: {tests['test_files']}\")\n    print(f\"  Test Directories: {len(tests['test_directories'])}\")\n    if tests['test_types']:\n        for test_type, count in tests['test_types'].items():\n            print(f\"    {test_type}: {count}\")\n    \n    # Code Quality\n    print(\"\\nâœ¨ CODE QUALITY\")\n    quality = analyze_code_quality(project_path)\n    print(f\"  Linting: {'âœ…' if quality['has_linting'] else 'âŒ'} {', '.join(quality['linters']) if quality['linters'] else 'None'}\")\n    print(f\"  Formatting: {'âœ…' if quality['has_formatting'] else 'âŒ'} {', '.join(quality['formatters']) if quality['formatters'] else 'None'}\")\n    print(f\"  Type Checking: {'âœ…' if quality['has_type_checking'] else 'âŒ'}\")\n    print(f\"  Documentation: {'âœ…' if quality['has_documentation'] else 'âŒ'}\")\n    \n    print(\"\\n\" + \"=\"*60 + \"\\n\")\n\nif __name__ == '__main__':\n    import sys\n    project_path = sys.argv[1] if len(sys.argv) > 1 else '.'\n    generate_audit_report(project_path)\n
